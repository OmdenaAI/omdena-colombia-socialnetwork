{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysisColombiaChapter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfU1VmpqoryT",
        "outputId": "5d3dcc51-690a-45df-c863-ac3c5f1a693c"
      },
      "source": [
        "!pip3 install vaderSentiment\n",
        "!pip3 install flair"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting flair\n",
            "  Downloading flair-0.9-py3-none-any.whl (319 kB)\n",
            "\u001b[K     |████████████████████████████████| 319 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 45.6 MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 48.4 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.0.0\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 51.7 MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.9.0+cu102)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.10.tar.gz (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Collecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 5.9 MB/s \n",
            "\u001b[?25hCollecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.62.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.6.3)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 660 kB/s \n",
            "\u001b[?25hCollecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 67.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, segtok, sqlitedict, ftfy, langdetect, wikipedia-api\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9704 sha256=120b1c82eec7409e0f4c6cf5ce400fe1131c5bc1ce2ad26df6764861724cd5d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=cea1abf1727b3d72fdd86d6fbb4066b2b86ca23502477d11f76ac051fe2174ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=f6b8aa55a0189b2b2d27c6984fca41fd3a08b27d179b243d3842e7229bac05df\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25030 sha256=561a75997c1d3babd81d5d08a3413287d542281883f1810c791073b1676f4b14\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=8a6ec9ba97eb4056bf586546126e7e7c3b6116de16dbfb22ff7cd50d3a821e31\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=a9b62a7399e910ed9dc1df64973eee60e3a2c1e09798a329df1a0135ed44d98b\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=7d8944f0f58a42cf1612c6e92cb7211d0ab252ac8c5bf62181710241e8c73327\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13475 sha256=df39c1d1e5ffaccbf4748c77e2ff544ba539c8b453dfd973b6af08764509b051\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built gdown mpld3 overrides segtok sqlitedict ftfy langdetect wikipedia-api\n",
            "Installing collected packages: requests, importlib-metadata, tokenizers, sentencepiece, sacremoses, pyyaml, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, mpld3, more-itertools, langdetect, konoha, janome, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.1\n",
            "    Uninstalling importlib-metadata-4.8.1:\n",
            "      Successfully uninstalled importlib-metadata-4.8.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.9.0\n",
            "    Uninstalling more-itertools-8.9.0:\n",
            "      Successfully uninstalled more-itertools-8.9.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.9 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.0.17 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pyyaml-5.4.1 requests-2.26.0 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 transformers-4.10.2 wikipedia-api-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyiLQ4BZZLR-",
        "outputId": "5e4d6b68-22e1-4abd-929e-dc2222d7ae0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyItwLv0aRN5",
        "outputId": "f4185957-ecc2-4728-b1ae-ee065bd7c6c4"
      },
      "source": [
        "cd MyDrive/Omdena\\ Colombia\\ Chapter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Omdena Colombia Chapter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TMixE6xahzs"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZmmZG--aj-G"
      },
      "source": [
        "data = pd.read_csv(\"Merged_twitter_en_topic_modelling_without_hashtags.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVcsif0qamrf",
        "outputId": "d2a15f69-8284-44cc-bfff-da55cf7ef3dc"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4548, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAKd8ZrIatdx"
      },
      "source": [
        "text = data['preprocessed_data_without_hashtags']\n",
        "text = text.to_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r3FlX8zfN1J",
        "outputId": "9bae9b52-b02b-4ce8-80f5-6433cd6857bd"
      },
      "source": [
        "print(type(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgB4bvASbZ8y",
        "outputId": "b40ac8c1-2f56-4aa4-f1dd-d9f4b5e3d216"
      },
      "source": [
        "print(text.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  preprocessed_data_without_hashtags\n",
            "0  ['join', 'seminar', 'aim', 'support', 'trade',...\n",
            "1                           ['caliente', 'tweetazo']\n",
            "2                                           ['sumo']\n",
            "3                            ['hay', 'diocancancan']\n",
            "4                     ['con', 'mucha', 'conciencia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzaSeX1cbbQK"
      },
      "source": [
        "joined_text = []\n",
        "for i in range(text.shape[0]):\n",
        "  joined_text.append(re.sub(r'[\\'\\,\\[\\]]','',text['preprocessed_data_without_hashtags'].iloc[i]))\n",
        "\n",
        "text['Refined_Text'] = joined_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Hl8Ebaekz9"
      },
      "source": [
        "lang_list = []\n",
        "for i in range(text.shape[0]):\n",
        "  try:\n",
        "    lang_list.append(detect(text['Refined_Text'].iloc[i]))\n",
        "  except:\n",
        "    lang_list.append(None)\n",
        "\n",
        "count = 0\n",
        "for i in range(len(lang_list)):\n",
        "  if lang_list[i] == 'en':\n",
        "    count+=1\n",
        "print(count)\n",
        "print(count/text.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzK4Sni9b0OE",
        "outputId": "eaa5b8ee-49df-4af1-fca0-ad8020c67750"
      },
      "source": [
        "text.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4548, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "F1RYbthycxVJ",
        "outputId": "dd808d2a-d82c-4306-9bc4-4e7e39a5d78e"
      },
      "source": [
        "text.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_data_without_hashtags</th>\n",
              "      <th>Refined_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['join', 'seminar', 'aim', 'support', 'trade',...</td>\n",
              "      <td>join seminar aim support trade union activity ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['caliente', 'tweetazo']</td>\n",
              "      <td>caliente tweetazo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['sumo']</td>\n",
              "      <td>sumo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['hay', 'diocancancan']</td>\n",
              "      <td>hay diocancancan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['con', 'mucha', 'conciencia']</td>\n",
              "      <td>con mucha conciencia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  preprocessed_data_without_hashtags                                       Refined_Text\n",
              "0  ['join', 'seminar', 'aim', 'support', 'trade',...  join seminar aim support trade union activity ...\n",
              "1                           ['caliente', 'tweetazo']                                  caliente tweetazo\n",
              "2                                           ['sumo']                                               sumo\n",
              "3                            ['hay', 'diocancancan']                                   hay diocancancan\n",
              "4                     ['con', 'mucha', 'conciencia']                               con mucha conciencia"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rtxud0wfyDg"
      },
      "source": [
        "#TextBlob\n",
        "from textblob import TextBlob\n",
        "score_textBlob = []\n",
        "for i in range(text.shape[0]):\n",
        "  testimonial = TextBlob(text['Refined_Text'].iloc[i])\n",
        "  score_textBlob.append(testimonial.sentiment)\n",
        "\n",
        "text['score_textBlob'] = score_textBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6FPmoNMoGUq"
      },
      "source": [
        "#VADER\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "score_VADER = []\n",
        "for i in range(text.shape[0]):\n",
        "  vs = analyzer.polarity_scores(text['Refined_Text'].iloc[i])\n",
        "  score_VADER.append(str(vs))\n",
        "\n",
        "text['score_VADER'] = score_VADER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-3YPKKppT-j",
        "outputId": "91e57dab-b0a6-4add-83f3-dbe8cc6ae38f"
      },
      "source": [
        "#Flair\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "score_Flair = []\n",
        "for i in range(text.shape[0]):\n",
        "  sentence = Sentence(text['Refined_Text'].iloc[i])\n",
        "  classifier.predict(sentence)\n",
        "  score_Flair.append(sentence.labels)\n",
        "\n",
        "text['score_Flair'] = score_Flair"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-09-11 02:29:20,938 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n",
            "2021-09-11 02:29:34,682 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:29:53,144 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:06,282 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:16,157 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:27,157 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:27,199 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:29,974 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:30,072 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:30,074 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:44,735 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:45,242 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:48,844 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:30:53,218 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:01,365 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:02,032 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:02,034 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:03,286 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:03,979 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:04,067 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:06,969 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:07,443 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:07,522 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:23,658 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:27,631 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:36,210 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:36,515 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:37,867 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:37,868 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:38,248 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:39,646 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:39,699 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:53,087 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:31:56,337 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:32:12,371 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:32:12,767 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:32:45,557 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:32:46,777 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
            "2021-09-11 02:32:48,745 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NVmYtTw9g7CP",
        "outputId": "4b18dedf-5453-408b-a6da-23894e3ef099"
      },
      "source": [
        "text.head(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_data_without_hashtags</th>\n",
              "      <th>Refined_Text</th>\n",
              "      <th>score_textBlob</th>\n",
              "      <th>score_VADER</th>\n",
              "      <th>score_Flair</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['join', 'seminar', 'aim', 'support', 'trade',...</td>\n",
              "      <td>join seminar aim support trade union activity ...</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'comp...</td>\n",
              "      <td>[POSITIVE (0.942)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['caliente', 'tweetazo']</td>\n",
              "      <td>caliente tweetazo</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (1.0)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['sumo']</td>\n",
              "      <td>sumo</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.6003)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['hay', 'diocancancan']</td>\n",
              "      <td>hay diocancancan</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.8287)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['con', 'mucha', 'conciencia']</td>\n",
              "      <td>con mucha conciencia</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[POSITIVE (0.9088)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>['somos', 'continuidad']</td>\n",
              "      <td>somos continuidad</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[POSITIVE (0.7904)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>['somos', 'continuidad']</td>\n",
              "      <td>somos continuidad</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[POSITIVE (0.7904)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>['somos', 'continuidad']</td>\n",
              "      <td>somos continuidad</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[POSITIVE (0.7904)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>['somos', 'continuidad']</td>\n",
              "      <td>somos continuidad</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[POSITIVE (0.7904)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>['somos', 'continuidad']</td>\n",
              "      <td>somos continuidad</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[POSITIVE (0.7904)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>['toda', 'la', 'noticias', 'son', 'malas']</td>\n",
              "      <td>toda la noticias son malas</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.98)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>['preliminary', 'conclusion', 'autonomists', '...</td>\n",
              "      <td>preliminary conclusion autonomists may day big...</td>\n",
              "      <td>(0.0, 0.1)</td>\n",
              "      <td>{'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'comp...</td>\n",
              "      <td>[POSITIVE (0.7543)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>['autonomous', 'may', 'day', 'demonstration', ...</td>\n",
              "      <td>autonomous may day demonstration oberbarmen di...</td>\n",
              "      <td>(0.35, 0.55)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.896, 'pos': 0.104, 'comp...</td>\n",
              "      <td>[POSITIVE (0.7862)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>['may', 'day', 'repo', 'video', 'read', 'amp',...</td>\n",
              "      <td>may day repo video read amp watch</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.9962)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>['unlike', 'distant', 'time', 'certain', 'ener...</td>\n",
              "      <td>unlike distant time certain energy patch venez...</td>\n",
              "      <td>(0.057142857142857134, 0.4607142857142857)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'comp...</td>\n",
              "      <td>[POSITIVE (0.6341)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>['absolutely', 'heartbreaking', 'happen', 'iba...</td>\n",
              "      <td>absolutely heartbreaking happen ibagué hometow...</td>\n",
              "      <td>(0.08750000000000001, 0.55)</td>\n",
              "      <td>{'neg': 0.32, 'neu': 0.405, 'pos': 0.274, 'com...</td>\n",
              "      <td>[POSITIVE (0.8797)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>['empower', 'community', 'mexico', 'central', ...</td>\n",
              "      <td>empower community mexico central america deman...</td>\n",
              "      <td>(0.10634920634920635, 0.2841269841269841)</td>\n",
              "      <td>{'neg': 0.05, 'neu': 0.503, 'pos': 0.446, 'com...</td>\n",
              "      <td>[POSITIVE (0.9969)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>['green', 'acre', 'purple', 'food', 'holding',...</td>\n",
              "      <td>green acre purple food holding new york strip ...</td>\n",
              "      <td>(-0.03181818181818183, 0.37727272727272726)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.798)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>['yesterday', 'may', 'st', 'demo', 'frankfu', ...</td>\n",
              "      <td>yesterday may st demo frankfu fight demonstrat...</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.245, 'neu': 0.755, 'pos': 0.0, 'comp...</td>\n",
              "      <td>[NEGATIVE (0.9828)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>['national', 'strike']</td>\n",
              "      <td>national strike</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.6, 'neu': 0.4, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.9243)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>['national', 'strike']</td>\n",
              "      <td>national strike</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.6, 'neu': 0.4, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.9243)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>['medina', 'spirit']</td>\n",
              "      <td>medina spirit</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.37, 'pos': 0.63, 'compou...</td>\n",
              "      <td>[POSITIVE (0.9745)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>['amo', 'en', 'toda', 'mi', 'versiones']</td>\n",
              "      <td>amo en toda mi versiones</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[POSITIVE (0.6883)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>['communist', 'march']</td>\n",
              "      <td>communist march</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.998)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>['calle', 'con']</td>\n",
              "      <td>calle con</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.9004)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>['police', 'fire', 'tear', 'gas']</td>\n",
              "      <td>police fire tear gas</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.444, 'neu': 0.556, 'pos': 0.0, 'comp...</td>\n",
              "      <td>[NEGATIVE (0.7866)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>['member', 'turn', 'street', 'war', 'zone']</td>\n",
              "      <td>member turn street war zone</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.494, 'neu': 0.506, 'pos': 0.0, 'comp...</td>\n",
              "      <td>[POSITIVE (0.9655)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>['police', 'clash']</td>\n",
              "      <td>police clash</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.9253)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>['difundan']</td>\n",
              "      <td>difundan</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.996)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>['difundir']</td>\n",
              "      <td>difundir</td>\n",
              "      <td>(0.0, 0.0)</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>[NEGATIVE (0.9996)]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   preprocessed_data_without_hashtags  ...          score_Flair\n",
              "0   ['join', 'seminar', 'aim', 'support', 'trade',...  ...   [POSITIVE (0.942)]\n",
              "1                            ['caliente', 'tweetazo']  ...     [NEGATIVE (1.0)]\n",
              "2                                            ['sumo']  ...  [NEGATIVE (0.6003)]\n",
              "3                             ['hay', 'diocancancan']  ...  [NEGATIVE (0.8287)]\n",
              "4                      ['con', 'mucha', 'conciencia']  ...  [POSITIVE (0.9088)]\n",
              "5                            ['somos', 'continuidad']  ...  [POSITIVE (0.7904)]\n",
              "6                            ['somos', 'continuidad']  ...  [POSITIVE (0.7904)]\n",
              "7                            ['somos', 'continuidad']  ...  [POSITIVE (0.7904)]\n",
              "8                            ['somos', 'continuidad']  ...  [POSITIVE (0.7904)]\n",
              "9                            ['somos', 'continuidad']  ...  [POSITIVE (0.7904)]\n",
              "10         ['toda', 'la', 'noticias', 'son', 'malas']  ...    [NEGATIVE (0.98)]\n",
              "11  ['preliminary', 'conclusion', 'autonomists', '...  ...  [POSITIVE (0.7543)]\n",
              "12  ['autonomous', 'may', 'day', 'demonstration', ...  ...  [POSITIVE (0.7862)]\n",
              "13  ['may', 'day', 'repo', 'video', 'read', 'amp',...  ...  [NEGATIVE (0.9962)]\n",
              "14  ['unlike', 'distant', 'time', 'certain', 'ener...  ...  [POSITIVE (0.6341)]\n",
              "15  ['absolutely', 'heartbreaking', 'happen', 'iba...  ...  [POSITIVE (0.8797)]\n",
              "16  ['empower', 'community', 'mexico', 'central', ...  ...  [POSITIVE (0.9969)]\n",
              "17  ['green', 'acre', 'purple', 'food', 'holding',...  ...   [NEGATIVE (0.798)]\n",
              "18  ['yesterday', 'may', 'st', 'demo', 'frankfu', ...  ...  [NEGATIVE (0.9828)]\n",
              "19                             ['national', 'strike']  ...  [NEGATIVE (0.9243)]\n",
              "20                             ['national', 'strike']  ...  [NEGATIVE (0.9243)]\n",
              "21                               ['medina', 'spirit']  ...  [POSITIVE (0.9745)]\n",
              "22           ['amo', 'en', 'toda', 'mi', 'versiones']  ...  [POSITIVE (0.6883)]\n",
              "23                             ['communist', 'march']  ...   [NEGATIVE (0.998)]\n",
              "24                                   ['calle', 'con']  ...  [NEGATIVE (0.9004)]\n",
              "25                  ['police', 'fire', 'tear', 'gas']  ...  [NEGATIVE (0.7866)]\n",
              "26        ['member', 'turn', 'street', 'war', 'zone']  ...  [POSITIVE (0.9655)]\n",
              "27                                ['police', 'clash']  ...  [NEGATIVE (0.9253)]\n",
              "28                                       ['difundan']  ...   [NEGATIVE (0.996)]\n",
              "29                                       ['difundir']  ...  [NEGATIVE (0.9996)]\n",
              "\n",
              "[30 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rivixtdg9WF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8144d861-d20d-4192-fd95-84b0f0703c89"
      },
      "source": [
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "sentence = Sentence('member turn street war zone')\n",
        "classifier.predict(sentence)\n",
        "\n",
        "# print sentence with predicted labels\n",
        "print('Sentence above is: ', sentence.labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-09-11 02:28:10,204 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n",
            "Sentence above is:  [POSITIVE (0.9655)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykUdIDBcrwFP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}