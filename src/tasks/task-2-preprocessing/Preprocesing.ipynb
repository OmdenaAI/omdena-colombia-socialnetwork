{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocesing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "skQ9HUfzrj1I",
        "ZUf1cNzRoxxZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Oq16Btg6sY"
      },
      "source": [
        "\n",
        "# Preproccesing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t2lw3iXoM0Y"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUqgzlw5hmu5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
        "from gensim.models.wrappers import LdaMallet\n",
        "from gensim.corpora import Dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-yFIGf4oSeD"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbAJxD7wmuuY"
      },
      "source": [
        "df_twitter = pd.read_csv(\"data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4ppE-O6oeOQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33VT9KItofMA"
      },
      "source": [
        "## Review dataset and select column of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "H982CO_Rm1vA",
        "outputId": "e986dfdd-e3b0-4860-b25a-113a9f7e783a"
      },
      "source": [
        "df_twitter.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>id_str</th>\n",
              "      <th>conversation_id_str</th>\n",
              "      <th>full_text</th>\n",
              "      <th>lang</th>\n",
              "      <th>favorited</th>\n",
              "      <th>retweeted</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>quote_count</th>\n",
              "      <th>quoted_status_id_str</th>\n",
              "      <th>quoted_status_short_url</th>\n",
              "      <th>quoted_status_expand_url</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>user_name</th>\n",
              "      <th>user_full_name</th>\n",
              "      <th>user_verified</th>\n",
              "      <th>in_reply_to_status_id_str</th>\n",
              "      <th>in_reply_to_user_id_str</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>mentions</th>\n",
              "      <th>urls</th>\n",
              "      <th>media</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-15T18:27:35+00:00</td>\n",
              "      <td>1393634179930214401</td>\n",
              "      <td>1393634179930214401</td>\n",
              "      <td>üíõ‚ù§Ô∏è R E S I S T E N C I A üíõüíô‚ù§Ô∏è\\r\\nQueremos un ...</td>\n",
              "      <td>es</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1389389028785631235</td>\n",
              "      <td>Distritoestbog</td>\n",
              "      <td>Distritoestudiantil</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#ParoNacional , #ParoNacional15M , #ColombiaSO...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[{\"url\": \"https://pbs.twimg.com/ext_tw_video_t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-05-15T14:03:51+00:00</td>\n",
              "      <td>1393567807632195584</td>\n",
              "      <td>1393408901299703812</td>\n",
              "      <td>@PaulinaVLackner @Ladelascuentas @RamirezAnlle...</td>\n",
              "      <td>es</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>702564560608370688</td>\n",
              "      <td>sontohi</td>\n",
              "      <td>Sonia</td>\n",
              "      <td>False</td>\n",
              "      <td>1.393464e+18</td>\n",
              "      <td>606068813.0</td>\n",
              "      <td>#ColombiaDDHH</td>\n",
              "      <td>PaulinaVLackner , Ladelascuentas , RamirezAnll...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  created_at  ...                                              media\n",
              "0  2021-05-15T18:27:35+00:00  ...  [{\"url\": \"https://pbs.twimg.com/ext_tw_video_t...\n",
              "1  2021-05-15T14:03:51+00:00  ...                                                 []\n",
              "\n",
              "[2 rows x 24 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlaJKLf_nMCT",
        "outputId": "08c9f623-9ac7-466b-baa4-4f222b1234b3"
      },
      "source": [
        "print(df_twitter['full_text'][3])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No s√© d√≥nde es, pero que valent√≠a la de los j√≥venes a quienes deber√≠a escuchar el estado.\r\n",
            "#NosEstanMantando\r\n",
            "#ColombiaDDHH \r\n",
            "@nytimes @UnivisionNews @bbcmundo @CNN @ABC  @CorteIDH @ONU_derechos @AmnistiaOnline @ONUHumanRights @cerosetenta @ONU_derechos  @CIDH  @TembloresOng https://t.co/ok6OAjCCBh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ronSCgqRn8Kw"
      },
      "source": [
        "df_text = df_twitter['full_text']\n",
        "df_text = df_text.to_frame()"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rKmPgTK-oByJ",
        "outputId": "6f73aba8-be99-49f4-fafc-c5f84a78a83e"
      },
      "source": [
        "df_text.head(5)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>üíõ‚ù§Ô∏è R E S I S T E N C I A üíõüíô‚ù§Ô∏è\\r\\nQueremos un ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@PaulinaVLackner @Ladelascuentas @RamirezAnlle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@RamirezAnllel abuso del poder #ColombiaDDHH \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No s√© d√≥nde es, pero que valent√≠a la de los j√≥...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@edladino_ @jclopezcastri @intiasprilla @Pizar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           full_text\n",
              "0  üíõ‚ù§Ô∏è R E S I S T E N C I A üíõüíô‚ù§Ô∏è\\r\\nQueremos un ...\n",
              "1  @PaulinaVLackner @Ladelascuentas @RamirezAnlle...\n",
              "2  @RamirezAnllel abuso del poder #ColombiaDDHH \\...\n",
              "3  No s√© d√≥nde es, pero que valent√≠a la de los j√≥...\n",
              "4  @edladino_ @jclopezcastri @intiasprilla @Pizar..."
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUf1cNzRoxxZ"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvaHso4xiL4Y"
      },
      "source": [
        "\n",
        "\n",
        "We defined a list of custom words to be exclude from our dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUgzx-3qg4r6"
      },
      "source": [
        "black_list = ['m√°s', 'mas', 'unir', 'paises', 'pais', 'espa', 'no', 'os', 'a', 'compa', 'acompa', 'off', 'and', 'grecia', 'the','it', 'to',\n",
        "              'd',  'et',  'dame',  'il',  'dans', 'that',  'as',   'for',  'it',  'elections',  'would',  'this',  'with', 'york', 'obama', 'chavez', 'gadafi']"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsC-L7i-iJl5"
      },
      "source": [
        "\n",
        "\n",
        "Create the cleaner function to clean the spanish text, remove non alpha numeric characters, remove duplicate, remove spanish accutes, remove digits\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDimujXngxmY"
      },
      "source": [
        "def cleaner(word):\n",
        "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', '', word, flags=re.MULTILINE)\n",
        "  word = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', \"\", word)\n",
        "  word = re.sub(r'ee.uu', 'eeuu', word)\n",
        "  word = re.sub(r'\\#\\.', '', word)\n",
        "  word = re.sub(r'\\n', '', word)\n",
        "  word = re.sub(r',', '', word)\n",
        "  word = re.sub(r'\\-', ' ', word)\n",
        "  word = re.sub(r'\\.{3}', ' ', word)\n",
        "  word = re.sub(r'a{2,}', 'a', word)\n",
        "  word = re.sub(r'√©{2,}', '√©', word)\n",
        "  word = re.sub(r'i{2,}', 'i', word)\n",
        "  word = re.sub(r'ja{2,}', 'ja', word) \n",
        "  word = re.sub(r'√°', 'a', word)\n",
        "  word = re.sub(r'√©', 'e', word)\n",
        "  word = re.sub(r'√≠', 'i', word)\n",
        "  word = re.sub(r'√≥', 'o', word)\n",
        "  word = re.sub(r'√∫', 'u', word)  \n",
        "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
        "  list_word_clean = []\n",
        "  for w1 in word.split(\" \"):\n",
        "    if  w1.lower() not in stopwords:\n",
        "      list_word_clean.append(w1.lower())\n",
        "\n",
        "  bigram_list = bigram[list_word_clean]\n",
        "  out_text = lemmatization(\" \".join(bigram_list))\n",
        "  return out_text"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MPPlHXeiCE6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Create the function for select only nouns for our data, this way we are removing adverb, adjetives, verbs, etc. This is doing with spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuwylKTwhnY5"
      },
      "source": [
        "def lemmatization(texts, allowed_postags=['NOUN']):\n",
        "    texts_out = [ token.text for token in nlp(texts) if token.pos_ in \n",
        "                 allowed_postags and token.text not in black_list and len(token.text)>2]\n",
        "    return texts_out\n",
        "    "
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFnAbRHiD3cJ",
        "outputId": "e152e0c2-dfec-4ae4-f602-56ccc8d25127"
      },
      "source": [
        "len(df_text)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "699"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEt1OFw9id3o"
      },
      "source": [
        "\n",
        "\n",
        "Then we look for bigram in our data, i.e pair of words that together have more meanful for our model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddVk00x-hobJ"
      },
      "source": [
        "%%time\n",
        "bigram = gensim.models.Phrases(df_text.full_text.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYkH_qAvikOq"
      },
      "source": [
        "bigram[df_text.full_text.to_list()[0].split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX4zMlWVJFpO",
        "outputId": "8bd93c1a-1df2-413a-b043-2257e684c15e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NkHe_fIiqMY"
      },
      "source": [
        "*Let's clean all the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8xy93PEipMJ"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('spanish'))\n",
        "additional_stopwords=set(black_list)\n",
        "stopwords = stop.union(additional_stopwords)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2hpLnzZJSIt"
      },
      "source": [
        "\n",
        "\n",
        "We are using Spacy For lemmatize spanish words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsDtLiHvqIBo"
      },
      "source": [
        "!python -m spacy download es_core_news_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8b__OzFJeUN"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('/usr/local/lib/python3.7/dist-packages/es_core_news_md/es_core_news_md-2.2.5')"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmB248tLJdiF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "TMlNWdx0Jwyv",
        "outputId": "569675de-7ff5-4942-92cc-42dc71e892ab"
      },
      "source": [
        "df_text.full_text[3]"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'No s√© d√≥nde es, pero que valent√≠a la de los j√≥venes a quienes deber√≠a escuchar el estado.\\r\\n#NosEstanMantando\\r\\n#ColombiaDDHH \\r\\n@nytimes @UnivisionNews @bbcmundo @CNN @ABC  @CorteIDH @ONU_derechos @AmnistiaOnline @ONUHumanRights @cerosetenta @ONU_derechos  @CIDH  @TembloresOng https://t.co/ok6OAjCCBh'"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmAW3ZQQKK2g",
        "outputId": "ab64bca1-55bb-4d4a-a296-a7a4e7f0421f"
      },
      "source": [
        "cleaner(df_text.full_text[3])"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['valentia']"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oILyDsfni0Mo",
        "outputId": "db61de0d-c16b-4a7f-838b-b9cecd1ab0f0"
      },
      "source": [
        "%%time\n",
        "df_text['cleaned'] = df_text.full_text.apply(cleaner)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8.2 s, sys: 20.2 ms, total: 8.22 s\n",
            "Wall time: 8.22 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ioc9-IqWN9EJ",
        "outputId": "aff1d023-6371-4c28-d1d1-aa12d97ab9fc"
      },
      "source": [
        "df_text"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "      <th>cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>üíõ‚ù§Ô∏è R E S I S T E N C I A üíõüíô‚ù§Ô∏è\\r\\nQueremos un ...</td>\n",
              "      <td>[futuro]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@PaulinaVLackner @Ladelascuentas @RamirezAnlle...</td>\n",
              "      <td>[abuso, poder]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@RamirezAnllel abuso del poder #ColombiaDDHH \\...</td>\n",
              "      <td>[poder]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No s√© d√≥nde es, pero que valent√≠a la de los j√≥...</td>\n",
              "      <td>[valentia]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@edladino_ @jclopezcastri @intiasprilla @Pizar...</td>\n",
              "      <td>[abuso, poder]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>@VenomRen @ARMANDO48997112 @Antonio29058651 @H...</td>\n",
              "      <td>[cuentas]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>@QuinteroCalle Sigan permitiendo estas barbari...</td>\n",
              "      <td>[barbaridades]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>@OJOVENE @jcy126kerubin @HUS07777781 @Samurai1...</td>\n",
              "      <td>[espacio, tuit, ataque, miseria, modo]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>@OJOVENE @jcy126kerubin @Fredypm @Florencia901...</td>\n",
              "      <td>[combate]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>Justicia para el pueblo Colombiano.\\r\\nCondena...</td>\n",
              "      <td>[pueblo, responsables]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>699 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             full_text                                 cleaned\n",
              "0    üíõ‚ù§Ô∏è R E S I S T E N C I A üíõüíô‚ù§Ô∏è\\r\\nQueremos un ...                                [futuro]\n",
              "1    @PaulinaVLackner @Ladelascuentas @RamirezAnlle...                          [abuso, poder]\n",
              "2    @RamirezAnllel abuso del poder #ColombiaDDHH \\...                                 [poder]\n",
              "3    No s√© d√≥nde es, pero que valent√≠a la de los j√≥...                              [valentia]\n",
              "4    @edladino_ @jclopezcastri @intiasprilla @Pizar...                          [abuso, poder]\n",
              "..                                                 ...                                     ...\n",
              "694  @VenomRen @ARMANDO48997112 @Antonio29058651 @H...                               [cuentas]\n",
              "695  @QuinteroCalle Sigan permitiendo estas barbari...                          [barbaridades]\n",
              "696  @OJOVENE @jcy126kerubin @HUS07777781 @Samurai1...  [espacio, tuit, ataque, miseria, modo]\n",
              "697  @OJOVENE @jcy126kerubin @Fredypm @Florencia901...                               [combate]\n",
              "698  Justicia para el pueblo Colombiano.\\r\\nCondena...                  [pueblo, responsables]\n",
              "\n",
              "[699 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxKT3uArOPoL"
      },
      "source": [
        "\n",
        "\n",
        "Now we need to build the corpus and the dictionary that gensim need to work, to do that we need to pass a list of list of tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POi8gk4Ti6q5"
      },
      "source": [
        "dictionary = Dictionary(df_text['cleaned'].to_list())\n",
        "dictionary.compactify()\n",
        "# Filter extremes\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.97, keep_n=None)\n",
        "dictionary.compactify()\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in df_text['cleaned'].to_list()]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPgS3wWlOTrp"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skQ9HUfzrj1I"
      },
      "source": [
        "## NLTK Preprocessing (not developed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-KlYfOernHL"
      },
      "source": [
        "text = \"Esta es una prueba de token con nltk objeto con t√≠ldes o tros 45 www.google.com #@ 321 de la casa\"\n",
        "text_token = word_tokenize(text)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtyIo8BisIvI",
        "outputId": "49e0e826-cdd9-4c55-f4a3-b34d24920f9b"
      },
      "source": [
        "text_token"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Esta',\n",
              " 'es',\n",
              " 'una',\n",
              " 'prueba',\n",
              " 'de',\n",
              " 'token',\n",
              " 'con',\n",
              " 'nltk',\n",
              " 'objeto',\n",
              " 'con',\n",
              " 't√≠ldes',\n",
              " 'o',\n",
              " 'tros',\n",
              " '45',\n",
              " 'www.google.com',\n",
              " '#',\n",
              " '@',\n",
              " '321',\n",
              " 'de',\n",
              " 'la',\n",
              " 'casa']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSQb0b1NsN6Z",
        "outputId": "6f406dc8-b503-4499-d048-30c1eb0ee8a7"
      },
      "source": [
        "token_wo_sw = [word for word in text_token if not word in stopwords.words('spanish')]\n",
        "print(token_wo_sw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Esta', 'prueba', 'token', 'nltk', 'objeto', 't√≠ldes', 'tros', '45', 'www.google.com', '#', '@', '321', 'casa']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrDi3D_dAhCP"
      },
      "source": [
        "## Preprocessing in sentiment analysis example (not tested)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DjWufTJAmfK"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.data import load\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "spanish_stopwords = stopwords.words('spanish')\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "non_words = list(punctuation)\n",
        "non_words.extend(['¬ø', '¬°'])\n",
        "non_words.extend(map(str,range(10)))\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmed = []\n",
        "    for item in tokens:\n",
        "        stemmed.append(stemmer.stem(item))\n",
        "    return stemmed\n",
        "def tokenize(text):\n",
        "    text = ''.join([c for c in text if c not in non_words])\n",
        "    tokens =  word_tokenize(text)\n",
        "    # stem\n",
        "    try:\n",
        "        stems = stem_tokens(tokens, stemmer)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(text)\n",
        "        stems = ['']\n",
        "    return stems\n"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjJlTlrHBPkD",
        "outputId": "5a4b5431-f22a-4ea4-decb-a022973d2b42"
      },
      "source": [
        "tokenize(text)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['esta',\n",
              " 'es',\n",
              " 'una',\n",
              " 'prueb',\n",
              " 'de',\n",
              " 'tok',\n",
              " 'con',\n",
              " 'nltk',\n",
              " 'objet',\n",
              " 'con',\n",
              " 'tild',\n",
              " 'o',\n",
              " 'tros',\n",
              " 'wwwgooglecom',\n",
              " 'de',\n",
              " 'la',\n",
              " 'cas']"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArBISDwUBXfl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}