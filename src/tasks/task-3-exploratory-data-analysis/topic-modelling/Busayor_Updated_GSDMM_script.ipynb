{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Topic Modeling with Gibbs Sampling Dirichlet Mixture Model (GSDMM)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GSDMM is a short-text topic modeling algorithm, which is a modified version of the LDA algorithm that uses the simple assumption of one topic assigned to a text.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The idea is simple. Imagine a professor is leading a film class of students. At the start of class, students are all asked to write down their favorite movies (relatively shortlist). The students represent documents and their movie lists represent words. Next, the students are randomly assigned to K tables. The goal is to cluster and group them in a way that students within the same table share similar movie interests. Lastly, the professor repeatedly reads the class roster; each time a student‚Äôs name is called, they must select a new table satisfying one or both of the following conditions:  \n",
    "> 1 ‚Äî Choose a table with more students than their current table.  \n",
    "> 2 ‚Äî Choose a table where students share similar movie interests. \n",
    "  \n",
    "Condition 1 improves **completeness**, all students with similar movie interests are at the same table rather than spread across different tables. Condition 2 helps lead to better **homogeneity**, ensuring that only members sharing similar interests are at the table. After satisfying these conditions and repeating them consistently until we near optimality (convergence), we expect some tables to disappear while others grow. The hope is that students will eventually arrive at an optimal table configuration. Simply, this is what the GSDMM algorithm does!   \n",
    "\n",
    "  \n",
    "Research paper can be found [here](https://dl.acm.org/doi/10.1145/2623330.2623715)  \n",
    "Medium article can be found here [here](https://pub.towardsai.net/tweet-topic-modeling-part-3-using-short-text-topic-modeling-on-tweets-bc969a827fef)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Code Implementation*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "clone github repo and import algorithm from file.  \n",
    "~~~\n",
    "git clone https://github.com/rwalk/gsdmm.git \n",
    "~~~"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gensim\n",
    "from gsdmm.gsdmm import MovieGroupProcess\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Read Data**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv('Corrected_Final_All.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(22160, 36)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>full_text</th>\n",
       "      <th>twitter_lang</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>preprocessed_data</th>\n",
       "      <th>emoji_list</th>\n",
       "      <th>emoticons_list</th>\n",
       "      <th>filename</th>\n",
       "      <th>data_source</th>\n",
       "      <th>lang</th>\n",
       "      <th>score</th>\n",
       "      <th>langTb</th>\n",
       "      <th>lang_langdetect</th>\n",
       "      <th>preprocessed_data_without_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-27T04:09:42+00:00</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>@Diputado_Canelo Hagamos otro por el uno de ma...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['hacer', 'mayo', 'cazar', 'fantasma', 'mayo']</td>\n",
       "      <td>['']</td>\n",
       "      <td>[':/']</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['hacer', 'mayo', 'cazar', 'fantasma']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-22T21:12:09+00:00</td>\n",
       "      <td>1.370000e+18</td>\n",
       "      <td>1.370000e+18</td>\n",
       "      <td>Despu√©s de esperar con ancias el #28F ahora es...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['despues', 'esperar', 'ancia', 'ahora', 'espe...</td>\n",
       "      <td>['üíôü§çüíô']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['despues', 'esperar', 'ancia', 'ahora', 'espe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-22T12:30:53+00:00</td>\n",
       "      <td>1.370000e+18</td>\n",
       "      <td>1.370000e+18</td>\n",
       "      <td>Espero que √©sto llegue hasta o√≠dos de la nueva...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['esperar', 'llegar', 'oido', 'nuevo', 'inicia...</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['esperar', 'llegar', 'oido', 'nuevo', 'inicia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-04-04T12:56:55+00:00</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>A menos de un mes del #1Mayo Urkullu teme perd...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['menos', 'mes', 'mayo', 'urkullu', 'temer', '...</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['menos', 'mes', 'urkullu', 'temer', 'perder',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-04-03T20:14:57+00:00</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>La X Edici√≥n del Festival Internacional Un Pue...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['edicion', 'festival', 'internacional', 'puen...</td>\n",
       "      <td>['']</td>\n",
       "      <td>[':/', ':/']</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['edicion', 'festival', 'internacional', 'puen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 created_at        id_str  conversation_id_str  \\\n",
       "0           0  2021-03-27T04:09:42+00:00  1.380000e+18         1.380000e+18   \n",
       "1           1  2021-03-22T21:12:09+00:00  1.370000e+18         1.370000e+18   \n",
       "2           2  2021-03-22T12:30:53+00:00  1.370000e+18         1.370000e+18   \n",
       "3           3  2021-04-04T12:56:55+00:00  1.380000e+18         1.380000e+18   \n",
       "4           4  2021-04-03T20:14:57+00:00  1.380000e+18         1.380000e+18   \n",
       "\n",
       "                                           full_text twitter_lang favorited  \\\n",
       "0  @Diputado_Canelo Hagamos otro por el uno de ma...           es     False   \n",
       "1  Despu√©s de esperar con ancias el #28F ahora es...           es     False   \n",
       "2  Espero que √©sto llegue hasta o√≠dos de la nueva...           es     False   \n",
       "3  A menos de un mes del #1Mayo Urkullu teme perd...           es     False   \n",
       "4  La X Edici√≥n del Festival Internacional Un Pue...           es     False   \n",
       "\n",
       "  retweeted  retweet_count  favorite_count  ...  \\\n",
       "0     False            0.0             1.0  ...   \n",
       "1     False            1.0             4.0  ...   \n",
       "2     False            0.0             1.0  ...   \n",
       "3     False            3.0             5.0  ...   \n",
       "4     False            1.0             3.0  ...   \n",
       "\n",
       "                                   preprocessed_data  emoji_list  \\\n",
       "0     ['hacer', 'mayo', 'cazar', 'fantasma', 'mayo']        ['']   \n",
       "1  ['despues', 'esperar', 'ancia', 'ahora', 'espe...     ['üíôü§çüíô']   \n",
       "2  ['esperar', 'llegar', 'oido', 'nuevo', 'inicia...        ['']   \n",
       "3  ['menos', 'mes', 'mayo', 'urkullu', 'temer', '...        ['']   \n",
       "4  ['edicion', 'festival', 'internacional', 'puen...        ['']   \n",
       "\n",
       "   emoticons_list                        filename  data_source  lang score  \\\n",
       "0          [':/']  Mayo_SPANISH_tweets_stweet.csv      Twitter    es   NaN   \n",
       "1              []  Mayo_SPANISH_tweets_stweet.csv      Twitter    es   NaN   \n",
       "2              []  Mayo_SPANISH_tweets_stweet.csv      Twitter    es   NaN   \n",
       "3              []  Mayo_SPANISH_tweets_stweet.csv      Twitter    es   NaN   \n",
       "4    [':/', ':/']  Mayo_SPANISH_tweets_stweet.csv      Twitter    es   NaN   \n",
       "\n",
       "  langTb lang_langdetect                 preprocessed_data_without_hashtags  \n",
       "0    NaN             NaN             ['hacer', 'mayo', 'cazar', 'fantasma']  \n",
       "1    NaN             NaN  ['despues', 'esperar', 'ancia', 'ahora', 'espe...  \n",
       "2    NaN             NaN  ['esperar', 'llegar', 'oido', 'nuevo', 'inicia...  \n",
       "3    NaN             NaN  ['menos', 'mes', 'urkullu', 'temer', 'perder',...  \n",
       "4    NaN             NaN  ['edicion', 'festival', 'internacional', 'puen...  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df['preprocessed_str_without_hashtags'] = df['preprocessed_data_without_hashtags'].apply(eval).apply(' '.join)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>full_text</th>\n",
       "      <th>twitter_lang</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>emoji_list</th>\n",
       "      <th>emoticons_list</th>\n",
       "      <th>filename</th>\n",
       "      <th>data_source</th>\n",
       "      <th>lang</th>\n",
       "      <th>score</th>\n",
       "      <th>langTb</th>\n",
       "      <th>lang_langdetect</th>\n",
       "      <th>preprocessed_data_without_hashtags</th>\n",
       "      <th>preprocessed_str_without_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-27T04:09:42+00:00</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>@Diputado_Canelo Hagamos otro por el uno de ma...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['']</td>\n",
       "      <td>[':/']</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['hacer', 'mayo', 'cazar', 'fantasma']</td>\n",
       "      <td>hacer mayo cazar fantasma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-22T21:12:09+00:00</td>\n",
       "      <td>1.370000e+18</td>\n",
       "      <td>1.370000e+18</td>\n",
       "      <td>Despu√©s de esperar con ancias el #28F ahora es...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['üíôü§çüíô']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['despues', 'esperar', 'ancia', 'ahora', 'espe...</td>\n",
       "      <td>despues esperar ancia ahora esperar despues se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-22T12:30:53+00:00</td>\n",
       "      <td>1.370000e+18</td>\n",
       "      <td>1.370000e+18</td>\n",
       "      <td>Espero que √©sto llegue hasta o√≠dos de la nueva...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['esperar', 'llegar', 'oido', 'nuevo', 'inicia...</td>\n",
       "      <td>esperar llegar oido nuevo iniciar laboral part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-04-04T12:56:55+00:00</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>A menos de un mes del #1Mayo Urkullu teme perd...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['menos', 'mes', 'urkullu', 'temer', 'perder',...</td>\n",
       "      <td>menos mes urkullu temer perder control dar pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-04-03T20:14:57+00:00</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>1.380000e+18</td>\n",
       "      <td>La X Edici√≥n del Festival Internacional Un Pue...</td>\n",
       "      <td>es</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['']</td>\n",
       "      <td>[':/', ':/']</td>\n",
       "      <td>Mayo_SPANISH_tweets_stweet.csv</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['edicion', 'festival', 'internacional', 'puen...</td>\n",
       "      <td>edicion festival internacional puente hacia ce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 created_at        id_str  conversation_id_str  \\\n",
       "0           0  2021-03-27T04:09:42+00:00  1.380000e+18         1.380000e+18   \n",
       "1           1  2021-03-22T21:12:09+00:00  1.370000e+18         1.370000e+18   \n",
       "2           2  2021-03-22T12:30:53+00:00  1.370000e+18         1.370000e+18   \n",
       "3           3  2021-04-04T12:56:55+00:00  1.380000e+18         1.380000e+18   \n",
       "4           4  2021-04-03T20:14:57+00:00  1.380000e+18         1.380000e+18   \n",
       "\n",
       "                                           full_text twitter_lang favorited  \\\n",
       "0  @Diputado_Canelo Hagamos otro por el uno de ma...           es     False   \n",
       "1  Despu√©s de esperar con ancias el #28F ahora es...           es     False   \n",
       "2  Espero que √©sto llegue hasta o√≠dos de la nueva...           es     False   \n",
       "3  A menos de un mes del #1Mayo Urkullu teme perd...           es     False   \n",
       "4  La X Edici√≥n del Festival Internacional Un Pue...           es     False   \n",
       "\n",
       "  retweeted  retweet_count  favorite_count  ...  emoji_list  emoticons_list  \\\n",
       "0     False            0.0             1.0  ...        ['']          [':/']   \n",
       "1     False            1.0             4.0  ...     ['üíôü§çüíô']              []   \n",
       "2     False            0.0             1.0  ...        ['']              []   \n",
       "3     False            3.0             5.0  ...        ['']              []   \n",
       "4     False            1.0             3.0  ...        ['']    [':/', ':/']   \n",
       "\n",
       "                         filename  data_source  lang  score langTb  \\\n",
       "0  Mayo_SPANISH_tweets_stweet.csv      Twitter    es    NaN    NaN   \n",
       "1  Mayo_SPANISH_tweets_stweet.csv      Twitter    es    NaN    NaN   \n",
       "2  Mayo_SPANISH_tweets_stweet.csv      Twitter    es    NaN    NaN   \n",
       "3  Mayo_SPANISH_tweets_stweet.csv      Twitter    es    NaN    NaN   \n",
       "4  Mayo_SPANISH_tweets_stweet.csv      Twitter    es    NaN    NaN   \n",
       "\n",
       "  lang_langdetect                 preprocessed_data_without_hashtags  \\\n",
       "0             NaN             ['hacer', 'mayo', 'cazar', 'fantasma']   \n",
       "1             NaN  ['despues', 'esperar', 'ancia', 'ahora', 'espe...   \n",
       "2             NaN  ['esperar', 'llegar', 'oido', 'nuevo', 'inicia...   \n",
       "3             NaN  ['menos', 'mes', 'urkullu', 'temer', 'perder',...   \n",
       "4             NaN  ['edicion', 'festival', 'internacional', 'puen...   \n",
       "\n",
       "                   preprocessed_str_without_hashtags  \n",
       "0                          hacer mayo cazar fantasma  \n",
       "1  despues esperar ancia ahora esperar despues se...  \n",
       "2  esperar llegar oido nuevo iniciar laboral part...  \n",
       "3  menos mes urkullu temer perder control dar pas...  \n",
       "4  edicion festival internacional puente hacia ce...  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pre-process data for modeling** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Put tokens in list format\n",
    "docs = df.preprocessed_data_without_hashtags.tolist()\n",
    "\n",
    "# remove punctuations\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "docs = list(sent_to_words(docs))\n",
    "docs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Train topic model**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Parameters**  \n",
    "\n",
    "> * *K -*  Represents the maximum number of topics to be found.  \n",
    "  \n",
    "> * **Alpha Œ± -** alpha controls a factor that decides how easily as table is removed when it is empty.  \n",
    "  \n",
    "> * **Beta Œ≤ -** beta controls how a table is chosen either based on similarity or popularity. low beta -> more similar clusters and high beta -> more emphasis on selecting popular clusters.  \n",
    "  \n",
    "> * **N_iters -** this represents the number of iterations or *number of times a student is reassigned to a new table by the proffessor*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Instantaite model\n",
    "mgp = MovieGroupProcess(K=12, alpha=0.1, beta=0.5, n_iters=30)\n",
    "\n",
    "# create vocab\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "\n",
    "# length of vocabulary\n",
    "n_terms = len(vocab)\n",
    "\n",
    "# fit on data\n",
    "y = mgp.fit(docs, n_terms)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In stage 0: transferred 19632 clusters with 12 clusters populated\n",
      "In stage 1: transferred 12433 clusters with 12 clusters populated\n",
      "In stage 2: transferred 4815 clusters with 12 clusters populated\n",
      "In stage 3: transferred 2558 clusters with 12 clusters populated\n",
      "In stage 4: transferred 1942 clusters with 12 clusters populated\n",
      "In stage 5: transferred 1788 clusters with 12 clusters populated\n",
      "In stage 6: transferred 1669 clusters with 12 clusters populated\n",
      "In stage 7: transferred 1548 clusters with 12 clusters populated\n",
      "In stage 8: transferred 1558 clusters with 12 clusters populated\n",
      "In stage 9: transferred 1517 clusters with 12 clusters populated\n",
      "In stage 10: transferred 1485 clusters with 11 clusters populated\n",
      "In stage 11: transferred 1469 clusters with 11 clusters populated\n",
      "In stage 12: transferred 1445 clusters with 11 clusters populated\n",
      "In stage 13: transferred 1408 clusters with 11 clusters populated\n",
      "In stage 14: transferred 1427 clusters with 11 clusters populated\n",
      "In stage 15: transferred 1394 clusters with 11 clusters populated\n",
      "In stage 16: transferred 1353 clusters with 11 clusters populated\n",
      "In stage 17: transferred 1368 clusters with 11 clusters populated\n",
      "In stage 18: transferred 1384 clusters with 11 clusters populated\n",
      "In stage 19: transferred 1345 clusters with 11 clusters populated\n",
      "In stage 20: transferred 1322 clusters with 11 clusters populated\n",
      "In stage 21: transferred 1256 clusters with 11 clusters populated\n",
      "In stage 22: transferred 1269 clusters with 11 clusters populated\n",
      "In stage 23: transferred 1259 clusters with 11 clusters populated\n",
      "In stage 24: transferred 1267 clusters with 12 clusters populated\n",
      "In stage 25: transferred 1225 clusters with 12 clusters populated\n",
      "In stage 26: transferred 1229 clusters with 11 clusters populated\n",
      "In stage 27: transferred 1241 clusters with 11 clusters populated\n",
      "In stage 28: transferred 1228 clusters with 11 clusters populated\n",
      "In stage 29: transferred 1231 clusters with 11 clusters populated\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# helper functions\n",
    "\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    '''prints the top words in each cluster'''\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print('Cluster %s : %s'%(cluster,sort_dicts))\n",
    "        print(' ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî')\n",
    "\n",
    "def cluster_importance(mgp):\n",
    "    '''returns a word-topic matrix[phi] where each value represents\n",
    "    the word importance for that particular cluster;\n",
    "    phi[i][w] would be the importance of word w in topic i.\n",
    "    '''\n",
    "    n_z_w = mgp.cluster_word_distribution\n",
    "    beta, V, K = mgp.beta, mgp.vocab_size, mgp.K\n",
    "    phi = [{} for i in range(K)]\n",
    "    for z in range(K):\n",
    "        for w in n_z_w[z]:\n",
    "            phi[z][w] = (n_z_w[z][w]+beta)/(sum(n_z_w[z].values())+V*beta)\n",
    "    return phi\n",
    "\n",
    "def topic_allocation(df, docs, mgp, topic_dict):\n",
    "    '''allocates all topics to each document in original dataframe,\n",
    "    adding two columns for cluster number and cluster description'''\n",
    "    topic_allocations = []\n",
    "    for doc in tqdm(docs):\n",
    "        topic_label, score = mgp.choose_best_label(doc)\n",
    "        topic_allocations.append(topic_label)\n",
    "\n",
    "    df['cluster'] = topic_allocations\n",
    "\n",
    "    df['topic_name'] = df.cluster.apply(lambda x: get_topic_name(x, topic_dict))\n",
    "    print('Complete. Number of documents with topic allocated: {}'.format(len(df)))\n",
    "\n",
    "def get_topic_name(doc, topic_dict):\n",
    "    '''returns the topic name string value from a dictionary of topics'''\n",
    "    topic_desc = topic_dict[doc]\n",
    "    return topic_desc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# topics sorted by the number of documents they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):',   \n",
    "       top_index)\n",
    "print('*'*20)\n",
    "# show the top 5 words in term frequency for each cluster \n",
    "topic_indices = np.arange(start=0, stop=len(doc_count), step=1)\n",
    "top_words(mgp.cluster_word_distribution, topic_indices, 10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of documents per topic : [   18    65   105 16085  4376    16     0  1053    21    24    12   385]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [ 3  4  7 11  2  1  9  8  0  5]\n",
      "********************\n",
      "Cluster 0 : [('post', 10), ('sub', 10), ('spam', 10), ('tropical', 6), ('severe', 6), ('por', 5), ('de', 5), ('regla', 5), ('due', 5), ('violation', 5)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 1 : [('mas', 40), ('el', 20), ('colombiano', 20), ('ayudar', 20), ('sr', 20), ('sostener', 20), ('volver', 19), ('casa', 19), ('mayor', 19), ('auxilio', 19)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 2 : [('uribe', 63), ('duque', 57), ('martuchis', 57), ('vicky', 55), ('paraco', 55), ('velez', 43), ('roman', 43), ('bolivar', 40), ('escobar', 39), ('polo', 39)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 3 : [('mas', 2028), ('hacer', 1772), ('si', 1730), ('el', 1587), ('ir', 1350), ('poder', 1339), ('pais', 1274), ('colombio', 1204), ('ser', 1201), ('gobierno', 1151)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 4 : [('colombia', 870), ('people', 739), ('police', 640), ('government', 528), ('help', 477), ('colombian', 468), ('protest', 404), ('kill', 365), ('right', 355), ('amp', 311)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 5 : [('is', 14), ('usar', 14), ('sub', 14), ('to', 14), ('lenguaje', 14), ('controversial', 14), ('language', 14), ('comunidad', 9), ('your', 8), ('not', 8)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 6 : []\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 7 : [('pm', 82), ('la', 79), ('hoy', 60), ('dialogo', 58), ('salud', 44), ('esperar', 43), ('calles', 38), ('colombio', 37), ('mayo', 37), ('de', 37)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 8 : [('of', 6), ('account', 6), ('post', 6), ('reddit', 6), ('or', 6), ('twitter', 5), ('temporarily', 5), ('unavailable', 5), ('violates', 5), ('policy', 5)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 9 : [('trabajador', 14), ('empresa', 14), ('ver', 14), ('encontrar', 14), ('oportunidad', 14), ('situacion', 14), ('pais', 14), ('afectado', 14), ('criticar', 14), ('mover', 14)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 10 : [('adi', 6), ('operador', 6), ('apdi', 6), ('jose', 6), ('ignacio', 6), ('briceno', 6), ('mendez', 6), ('hezbollah', 6), ('colombiano', 4), ('abril', 3)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n",
      "Cluster 11 : [('mas', 284), ('si', 258), ('hacer', 210), ('el', 200), ('people', 199), ('poder', 198), ('ir', 167), ('ser', 162), ('decir', 156), ('colombia', 145)]\n",
      " ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "phi = cluster_importance(mgp) # initialize phi matrix\n",
    "phi[4]['government']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.00887131970322624"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "phi[8]['policy']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0004564694165490912"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Save model\n",
    "with open('12cluster.model', 'wb') as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def top_words_dict(cluster_word_distribution, top_cluster, n_words):\n",
    "    '''returns a dictionary of the top n words and the number of docs they are in;\n",
    "    cluster numbers are the keys and a tuple of (word, word count) are the values'''\n",
    "    top_words_dict = {}\n",
    "    for cluster in top_cluster:\n",
    "        top_words_list = []\n",
    "        for val in range(1, n_words):\n",
    "            top_n_word = sorted(cluster_word_distribution[cluster].items(), \n",
    "                                key=lambda item: item[1], reverse=True)[:n_words][val]    #[0]\n",
    "            top_words_list.append(top_n_word)\n",
    "        top_words_dict[cluster] = top_words_list\n",
    "\n",
    "    return top_words_dict\n",
    "\n",
    "def get_word_counts_dict(top_words_nclusters):\n",
    "    '''returns a dictionary that counts the number of times a word \n",
    "    appears only in the top n words list across all the clusters;\n",
    "    words are the keys and a count of the word is the value'''\n",
    "    word_count_dict = {}\n",
    "    for key in top_words_nclusters:\n",
    "        words_score_list = []\n",
    "        for word in top_words_nclusters[key]:\n",
    "            if word[0] in word_count_dict.keys():\n",
    "                word_count_dict[word[0]] += 1\n",
    "            else:\n",
    "                word_count_dict[word[0]] = 1\n",
    "    return word_count_dict\n",
    "\n",
    "def get_cluster_importance_dict(top_words_nclusters, phi):\n",
    "    '''returns a dictionary that of all top words and their cluster\n",
    "    importance value for each cluster;\n",
    "    cluster numbers are the keys and a list of word \n",
    "    importance computed scores are the values'''\n",
    "    cluster_importance_dict = {}\n",
    "    for key in top_words_nclusters:\n",
    "        words_score_list = []\n",
    "        for word in top_words_nclusters[key]:\n",
    "            importance_score = phi[key][word[0]]\n",
    "            words_score_list.append(importance_score)\n",
    "        cluster_importance_dict[key] = words_score_list\n",
    "    return cluster_importance_dict\n",
    "\n",
    "def get_doc_counts_dict(top_words_nclusters):\n",
    "    '''returns a dictionary of only the doc counts of each top n word for each cluster;\n",
    "    cluster numbers are the keys and a list of doc counts are the values'''\n",
    "    doc_counts_dict = {}\n",
    "    for key in top_words_nclusters:\n",
    "        doc_counts_list = []\n",
    "        for word in top_words_nclusters[key]:\n",
    "            num_docs = word[1]\n",
    "            doc_counts_list.append(num_docs)\n",
    "        doc_counts_dict[key] = doc_counts_list\n",
    "    return doc_counts_dict\n",
    "\n",
    "def get_word_frequency_dict(top_words_nclusters, word_counts):\n",
    "    '''returns a dictionary of only the number of occurences across all \n",
    "    clusters for each word in a particular cluster's top n words;\n",
    "    cluster numbers are the keys and a list of \n",
    "    word occurences counts are the values'''\n",
    "    word_frequency_dict = {}\n",
    "    for key in top_words_nclusters:\n",
    "        words_count_list = []\n",
    "        for word in top_words_nclusters[key]:\n",
    "            words_count_list.append(word_counts[word[0]])\n",
    "        word_frequency_dict[key] = words_count_list\n",
    "\n",
    "    return word_frequency_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "\n",
    "# declare any static variables needed \n",
    "nwords = 10\n",
    "phi = cluster_importance(mgp)\n",
    "modified_topic_indices = np.delete(topic_indices, 6) # topic 6 is an empty table\n",
    "# define and generate dictionaries that hold each topic number and its values\n",
    "top_words = top_words_dict(mgp.cluster_word_distribution, modified_topic_indices, nwords)\n",
    "word_count = get_word_counts_dict(top_words)\n",
    "word_frequency = get_word_frequency_dict(top_words, word_count)\n",
    "cluster_importance_dict = get_cluster_importance_dict(top_words, phi)\n",
    "    \n",
    "# add all values for each topic to a list of lists\n",
    "rows_list = []\n",
    "for cluster in range(0, 10):\n",
    "    if cluster == 6:\n",
    "        pass\n",
    "    else:\n",
    "        words = [x[0] for x in top_words[cluster]]\n",
    "        doc_counts = [x[1] for x in top_words[cluster]]\n",
    "    \n",
    "    # create a list of values which represents a 'row' in our data frame \n",
    "        rows_list.append([int(cluster), words, doc_counts, \n",
    "                        word_frequency[cluster], cluster_importance_dict[cluster]])\n",
    "        \n",
    "topic_words_df = pd.DataFrame(data=rows_list, \n",
    "                              columns=['cluster', 'top_words',\n",
    "                                        'doc_count', 'num_topic_occurrence', 'word_importance'])\n",
    "\n",
    "topic_words_df\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>top_words</th>\n",
       "      <th>doc_count</th>\n",
       "      <th>num_topic_occurrence</th>\n",
       "      <th>word_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[sub, spam, tropical, severe, por, de, regla, ...</td>\n",
       "      <td>[10, 10, 6, 6, 5, 5, 5, 5, 5]</td>\n",
       "      <td>[2, 1, 1, 1, 1, 2, 1, 1, 1]</td>\n",
       "      <td>[0.0008700696055684454, 0.0008700696055684454,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[el, colombiano, ayudar, sr, sostener, volver,...</td>\n",
       "      <td>[20, 20, 20, 20, 20, 19, 19, 19, 19]</td>\n",
       "      <td>[3, 2, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0.0016414444711345985, 0.0016414444711345985,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[duque, martuchis, vicky, paraco, velez, roman...</td>\n",
       "      <td>[57, 57, 55, 55, 43, 43, 40, 39, 39]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0.0044281863688871775, 0.0044281863688871775,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[hacer, si, el, ir, poder, pais, colombio, ser...</td>\n",
       "      <td>[1772, 1730, 1587, 1350, 1339, 1274, 1204, 120...</td>\n",
       "      <td>[2, 2, 3, 2, 2, 2, 2, 2, 1]</td>\n",
       "      <td>[0.009441147958368399, 0.009217436695038937, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[people, police, government, help, colombian, ...</td>\n",
       "      <td>[739, 640, 528, 477, 468, 404, 365, 355, 311]</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0.012413133246046933, 0.010751334474770873, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[usar, sub, to, lenguaje, controversial, langu...</td>\n",
       "      <td>[14, 14, 14, 14, 14, 14, 9, 8, 8]</td>\n",
       "      <td>[1, 2, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0.0011722855525911552, 0.0011722855525911552,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>[la, hoy, dialogo, salud, esperar, calles, col...</td>\n",
       "      <td>[79, 60, 58, 44, 43, 38, 37, 37, 37]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 2, 1, 2]</td>\n",
       "      <td>[0.004121733720447947, 0.0031366652841144753, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>[account, post, reddit, or, twitter, temporari...</td>\n",
       "      <td>[6, 6, 6, 6, 5, 5, 5, 5, 5]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0.0005394638559216532, 0.0005394638559216532,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>[empresa, ver, encontrar, oportunidad, situaci...</td>\n",
       "      <td>[14, 14, 14, 14, 14, 14, 14, 14, 14]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 2, 1, 1, 1]</td>\n",
       "      <td>[0.0012011265738899933, 0.0012011265738899933,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                          top_words  \\\n",
       "0        0  [sub, spam, tropical, severe, por, de, regla, ...   \n",
       "1        1  [el, colombiano, ayudar, sr, sostener, volver,...   \n",
       "2        2  [duque, martuchis, vicky, paraco, velez, roman...   \n",
       "3        3  [hacer, si, el, ir, poder, pais, colombio, ser...   \n",
       "4        4  [people, police, government, help, colombian, ...   \n",
       "5        5  [usar, sub, to, lenguaje, controversial, langu...   \n",
       "6        7  [la, hoy, dialogo, salud, esperar, calles, col...   \n",
       "7        8  [account, post, reddit, or, twitter, temporari...   \n",
       "8        9  [empresa, ver, encontrar, oportunidad, situaci...   \n",
       "\n",
       "                                           doc_count  \\\n",
       "0                      [10, 10, 6, 6, 5, 5, 5, 5, 5]   \n",
       "1               [20, 20, 20, 20, 20, 19, 19, 19, 19]   \n",
       "2               [57, 57, 55, 55, 43, 43, 40, 39, 39]   \n",
       "3  [1772, 1730, 1587, 1350, 1339, 1274, 1204, 120...   \n",
       "4      [739, 640, 528, 477, 468, 404, 365, 355, 311]   \n",
       "5                  [14, 14, 14, 14, 14, 14, 9, 8, 8]   \n",
       "6               [79, 60, 58, 44, 43, 38, 37, 37, 37]   \n",
       "7                        [6, 6, 6, 6, 5, 5, 5, 5, 5]   \n",
       "8               [14, 14, 14, 14, 14, 14, 14, 14, 14]   \n",
       "\n",
       "          num_topic_occurrence  \\\n",
       "0  [2, 1, 1, 1, 1, 2, 1, 1, 1]   \n",
       "1  [3, 2, 1, 1, 1, 1, 1, 1, 1]   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3  [2, 2, 3, 2, 2, 2, 2, 2, 1]   \n",
       "4  [2, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "5  [1, 2, 1, 1, 1, 1, 1, 1, 1]   \n",
       "6  [1, 1, 1, 1, 1, 1, 2, 1, 2]   \n",
       "7  [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "8  [1, 1, 1, 1, 1, 2, 1, 1, 1]   \n",
       "\n",
       "                                     word_importance  \n",
       "0  [0.0008700696055684454, 0.0008700696055684454,...  \n",
       "1  [0.0016414444711345985, 0.0016414444711345985,...  \n",
       "2  [0.0044281863688871775, 0.0044281863688871775,...  \n",
       "3  [0.009441147958368399, 0.009217436695038937, 0...  \n",
       "4  [0.012413133246046933, 0.010751334474770873, 0...  \n",
       "5  [0.0011722855525911552, 0.0011722855525911552,...  \n",
       "6  [0.004121733720447947, 0.0031366652841144753, ...  \n",
       "7  [0.0005394638559216532, 0.0005394638559216532,...  \n",
       "8  [0.0012011265738899933, 0.0012011265738899933,...  "
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# save results\n",
    "topic_words_df.to_csv('results.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "bac0d3b00910ae8e722a9cb408cffb85fff434fc598a7d0db82de63feda35b8c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}